{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\George\n",
      "[nltk_data]     Sidorov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\George\n",
      "[nltk_data]     Sidorov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import sys\n",
    "import emoji\n",
    "import docx2txt\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_n:\n",
    "\n",
    "    def __init__(self, initial_text=None):\n",
    "\n",
    "        self.initial_text = initial_text\n",
    "        self.initial_text_without_emoji = None\n",
    "\n",
    "        self.stemmed_text = None\n",
    "        self.tokenized_text = None\n",
    "\n",
    "        self.list_gramms_stemmed = []\n",
    "        self.list_gramms_tokens = []\n",
    "\n",
    "        self.list_unigramm_stemmed = []\n",
    "        self.list_unigramm_tokens = []\n",
    "\n",
    "        self.dict_gramms = {}\n",
    "        self.sorted_dict_gramms = {}\n",
    "\n",
    "        self.final_dict = {}\n",
    "\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    # ---------------------------- Clean initial text from emojis ------------------------------- #\n",
    "\n",
    "    def emoji_cleaner(self):\n",
    "        \n",
    "        allchars = [str for str in self.initial_text]\n",
    "        emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "        self.initial_text_without_emoji = ' '.join([str for str in self.initial_text.split() if not any(i in str for i in emoji_list)])\n",
    "            \n",
    "    # ---------------------------- Stemming initial text cleaned from emoji ------------------------------- #\n",
    "\n",
    "    def stemmer(self):\n",
    "\n",
    "        tokens = word_tokenize(self.initial_text_without_emoji, language=\"russian\")\n",
    "\n",
    "        tokens_lower = [i.lower() for i in tokens]\n",
    "\n",
    "        tokens_without_punctuation = [i for i in tokens_lower if i not in string.punctuation]\n",
    "        self.tokenized_text = [i for i in tokens_without_punctuation if i not in russian_stop_words]\n",
    "        snowball = SnowballStemmer(\"russian\")\n",
    "        self.stemmed_text = [snowball.stem(i) for i in self.tokenized_text]\n",
    "\n",
    "    # ---------------------------- Stemmed text cleaning and preparation ------------------------------- #\n",
    "\n",
    "    def text_cleaner(self):\n",
    "        \n",
    "        for item in self.stemmed_text:\n",
    "            if len(item) <= 3 or item in russian_stop_words:\n",
    "                \n",
    "                index = self.stemmed_text.index(item)\n",
    "\n",
    "                try:\n",
    "                    del self.stemmed_text[index]\n",
    "                    del self.tokenized_text[index]\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                \n",
    "        for item in self.stemmed_text:\n",
    "            if item in russian_stop_words:\n",
    "                self.stemmed_text.remove(item)\n",
    "                \n",
    "    # ---------------------------- Created N-gramms ------------------------------- #\n",
    "\n",
    "    def n_gramming(self, n):\n",
    "\n",
    "        ngrams_stemmed = ngrams(self.stemmed_text, n)\n",
    "        for grams in ngrams_stemmed:\n",
    "            self.list_gramms_stemmed.append(grams)\n",
    "\n",
    "        ngrams_tokens = ngrams(self.tokenized_text, n)\n",
    "        for grams in ngrams_tokens:\n",
    "            self.list_gramms_tokens.append(grams)\n",
    "\n",
    "    # ---------------------------- Find matches ------------------------------- #\n",
    "\n",
    "    def n_gramms_finder(self, n):\n",
    "\n",
    "        for stemmed_tuple in self.list_gramms_stemmed:\n",
    "\n",
    "            holder = []\n",
    "            counter = 0\n",
    "\n",
    "            for index, tup in enumerate(self.list_gramms_tokens):\n",
    "                flag = False\n",
    "\n",
    "                for item_stemmed, item_list in zip(stemmed_tuple, tup):\n",
    "                    if item_stemmed in item_list:\n",
    "                        flag = True\n",
    "                    else:\n",
    "                        flag = False\n",
    "                        break\n",
    "\n",
    "                if flag:\n",
    "                    counter += 1\n",
    "                    holder.append(tup)\n",
    "\n",
    "                if counter > 1:\n",
    "                    self.dict_gramms[stemmed_tuple] = (n, counter, holder)\n",
    "\n",
    "        sorted_values = sorted(self.dict_gramms.values(), reverse=True)\n",
    "\n",
    "        for i in sorted_values:\n",
    "            for k in self.dict_gramms.keys():\n",
    "                if self.dict_gramms[k] == i:\n",
    "                    self.sorted_dict_gramms[k] = self.dict_gramms[k]\n",
    "                    break\n",
    "\n",
    "    # ---------------------------- Exclude matched n - gramms from n gramm stemmed list ----------------------------- #\n",
    "\n",
    "    def n_gramms_stemmed_excluder(self, n):\n",
    "\n",
    "        for key in self.sorted_dict_gramms.keys():\n",
    "            for i in range(self.sorted_dict_gramms[key][1]):\n",
    "\n",
    "                try:\n",
    "                    index = self.list_gramms_stemmed.index(key)\n",
    "                    self.list_gramms_stemmed.remove(key)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                else:\n",
    "                    if index != 0 and index != len(self.list_gramms_stemmed):\n",
    "                        self.list_gramms_stemmed[index - 1] = list(self.list_gramms_stemmed[index - 1])\n",
    "                        self.list_gramms_stemmed[index] = list(self.list_gramms_stemmed[index])\n",
    "                        self.list_gramms_stemmed[index - 1] = self.list_gramms_stemmed[index - 1][0]\n",
    "                        self.list_gramms_stemmed[index] = self.list_gramms_stemmed[index][n - 1]\n",
    "                        self.list_unigramm_stemmed.append(self.list_gramms_stemmed[index - 1])\n",
    "                        self.list_unigramm_stemmed.append(self.list_gramms_stemmed[index])\n",
    "                        del self.list_gramms_stemmed[index]\n",
    "                        del self.list_gramms_stemmed[index - 1]\n",
    "\n",
    "                    if index == 0:\n",
    "                        self.list_gramms_stemmed[index] = list(self.list_gramms_stemmed[index])\n",
    "                        self.list_gramms_stemmed[index] = self.list_gramms_stemmed[index][n - 1]\n",
    "                        self.list_unigramm_stemmed.append(self.list_gramms_stemmed[index])\n",
    "                        del self.list_gramms_stemmed[index]\n",
    "\n",
    "                    if index == len(self.list_gramms_stemmed):\n",
    "                        self.list_gramms_stemmed[-1] = list(self.list_gramms_stemmed[-1])\n",
    "                        self.list_gramms_stemmed[-1] = self.list_gramms_stemmed[-1][0]\n",
    "                        self.list_unigramm_stemmed.append(self.list_gramms_stemmed[-1])\n",
    "                        del self.list_gramms_stemmed[-1]\n",
    "\n",
    "    # --------------------------- Exclude matched n gramms from n gramm tokenized list ---------------------------- #\n",
    "\n",
    "    def n_gramms_tokens_excluder(self, n):\n",
    "\n",
    "        for key in self.sorted_dict_gramms.keys():\n",
    "            for item in self.sorted_dict_gramms[key][2]:\n",
    "\n",
    "                try:\n",
    "                    index = self.list_gramms_tokens.index(item)\n",
    "                    self.list_gramms_tokens.remove(item)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                else:\n",
    "                    if index != 0 and index != len(self.list_gramms_tokens):\n",
    "                        self.list_gramms_tokens[index - 1] = list(self.list_gramms_tokens[index - 1])\n",
    "                        self.list_gramms_tokens[index] = list(self.list_gramms_tokens[index])\n",
    "                        self.list_gramms_tokens[index - 1] = self.list_gramms_tokens[index - 1][0]\n",
    "                        self.list_gramms_tokens[index] = self.list_gramms_tokens[index][n - 1]\n",
    "                        self.list_unigramm_tokens.append(self.list_gramms_tokens[index - 1])\n",
    "                        self.list_unigramm_tokens.append(self.list_gramms_tokens[index])\n",
    "                        del self.list_gramms_tokens[index]\n",
    "                        del self.list_gramms_tokens[index - 1]\n",
    "\n",
    "                    if index == 0:\n",
    "                        self.list_gramms_tokens[index] = list(self.list_gramms_tokens[index])\n",
    "                        self.list_gramms_tokens[index] = self.list_gramms_tokens[index][n - 1]\n",
    "                        self.list_unigramm_tokens.append(self.list_gramms_tokens[index])\n",
    "                        del self.list_gramms_tokens[index]\n",
    "\n",
    "                    if index == len(self.list_gramms_tokens):\n",
    "                        self.list_gramms_tokens[-1] = list(self.list_gramms_tokens[-1])\n",
    "                        self.list_gramms_tokens[-1] = self.list_gramms_tokens[-1][0]\n",
    "                        self.list_unigramm_tokens.append(self.list_gramms_tokens[-1])\n",
    "                        del self.list_gramms_tokens[-1]\n",
    "\n",
    "    # --------------------------- Removing duplicate n gramms ---------------------------- #\n",
    "\n",
    "    def cleaner(self, n):\n",
    "\n",
    "        self.list_gramms_stemmed = self.list_gramms_stemmed[0::n]\n",
    "        self.list_gramms_tokens = self.list_gramms_tokens[0::n]\n",
    "\n",
    "    def tuple_collector(self, text):\n",
    "\n",
    "        return \" \".join(text)\n",
    "\n",
    "    # --------------------------- Modifier from n gramms to text  ---------------------------- #\n",
    "\n",
    "    def text_collector(self):\n",
    "\n",
    "        self.stemmed_text = \" \".join(list(map(self.tuple_collector, self.list_gramms_stemmed)))\n",
    "        self.tokenized_text = \" \".join(list(map(self.tuple_collector, self.list_gramms_tokens)))\n",
    "\n",
    "        self.stemmed_text = self.stemmed_text.split()\n",
    "        self.tokenized_text = self.tokenized_text.split()\n",
    "\n",
    "    # --------------------------- Store and clean final dictionary  ---------------------------- #\n",
    "\n",
    "    def store_and_clean(self):\n",
    "\n",
    "        self.final_dict.update(self.sorted_dict_gramms)\n",
    "\n",
    "        self.dict_gramms = {}\n",
    "        self.sorted_dict_gramms = {}\n",
    "\n",
    "        self.list_gramms_stemmed = []\n",
    "        self.list_gramms_tokens = []\n",
    "\n",
    "    # --------------------------- Colect unigramms  ---------------------------- #\n",
    "\n",
    "    def collect_unigrams(self):\n",
    "\n",
    "        self.stemmed_text += self.list_unigramm_stemmed\n",
    "        self.tokenized_text += self.list_unigramm_tokens\n",
    "        \n",
    "    # ---------------------------- Unigramms cleaner --------------------------- #\n",
    "    \n",
    "    def unigramm_cleaner(self):\n",
    "        for item in self.list_unigramm_stemmed:\n",
    "\n",
    "            if len(item) <= 3 or item in russian_stop_words:\n",
    "                \n",
    "                index = self.list_unigramm_stemmed.index(item)\n",
    "\n",
    "                try:\n",
    "                    del self.list_unigramm_stemmed[index]\n",
    "                    del self.list_unigramm_tokens[index]\n",
    "                except IndexError:\n",
    "                    pass                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_collecter():\n",
    "    \n",
    "    with open(\"stop_words.txt\", mode=\"r\", encoding='utf-8') as stop_words_file:\n",
    "        russian_stop_words = stop_words_file.readlines()\n",
    "    for i in range(len(russian_stop_words) - 1):\n",
    "        russian_stop_words[i] = russian_stop_words[i][:-1]\n",
    "\n",
    "    return russian_stop_words\n",
    "\n",
    "def user_stop_words_collecter():\n",
    "    \n",
    "    with open(\"user_stop_words.txt\", mode=\"r\", encoding='utf-8') as user_stop_words_file:\n",
    "        user_stop_words = user_stop_words_file.readlines()\n",
    "    for i in range(len(user_stop_words) - 1):\n",
    "        user_stop_words[i] = user_stop_words[i][:-1]\n",
    "        \n",
    "    return user_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(n, r):\n",
    "    \n",
    "    my_text = docx2txt.process(\"text.docx\")\n",
    "    \n",
    "    is_on = True\n",
    "\n",
    "    first_n = n\n",
    "    text_processor = Text_n(my_text)\n",
    "    \n",
    "    while is_on:\n",
    "\n",
    "        if n == first_n:\n",
    "            text_processor.emoji_cleaner()\n",
    "            text_processor.stemmer()\n",
    "            text_processor.n_gramming(n)\n",
    "            text_processor.n_gramms_finder(n)\n",
    "            text_processor.n_gramms_stemmed_excluder(n)\n",
    "            text_processor.n_gramms_tokens_excluder(n)\n",
    "            text_processor.cleaner(n)\n",
    "            text_processor.text_collector()\n",
    "            text_processor.store_and_clean()\n",
    "            n -= 1\n",
    "\n",
    "        if n == 1:\n",
    "            text_processor.text_cleaner()\n",
    "            text_processor.unigramm_cleaner()\n",
    "            text_processor.collect_unigrams()\n",
    "            text_processor.n_gramming(n)\n",
    "            text_processor.n_gramms_finder(n)\n",
    "            text_processor.store_and_clean()\n",
    "            n -= 1\n",
    "\n",
    "        else:\n",
    "                  \n",
    "            text_processor.n_gramming(n)\n",
    "            text_processor.n_gramms_finder(n)\n",
    "            text_processor.n_gramms_stemmed_excluder(n)\n",
    "            text_processor.n_gramms_tokens_excluder(n)\n",
    "            text_processor.cleaner(n)\n",
    "            text_processor.text_collector()\n",
    "            text_processor.store_and_clean()\n",
    "            n -= 1\n",
    "\n",
    "        if n == 0:\n",
    "            is_on = False\n",
    "\n",
    "    column_1 = []\n",
    "    column_2 = []\n",
    "    column_3 = []\n",
    "    column_4 = []\n",
    "    \n",
    "    for item in text_processor.final_dict:\n",
    "        joined = \" \".join(item)\n",
    "        column_1.append(joined)\n",
    "        column_2.append(text_processor.final_dict[item][0])\n",
    "        column_3.append(text_processor.final_dict[item][1])\n",
    "        \n",
    "        all_expressions = []\n",
    "        \n",
    "        for i in text_processor.final_dict[item][2]:\n",
    "            joined_tuple = \" \".join(i)\n",
    "            all_expressions.append(joined_tuple)\n",
    "        column_4.append(\", \".join(all_expressions))\n",
    "\n",
    "    data = {\"N-gram\": column_1, \"N\": column_2, \"Number\": column_3, \"Expressions\": column_4}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.sort_values(['N-gram', 'Number'], ascending=[True, False])\n",
    "    df = df[df['Number'] > r].reset_index(drop=True)\n",
    "    file_name = \"N_grams.xlsx\"\n",
    "    df.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Собираем стоп-слова --------------------------- #\n",
    "russian_stop_words = stop_words_collecter()\n",
    "user_words = user_stop_words_collecter()\n",
    "russian_stop_words += user_words\n",
    "\n",
    "# ---------------------------- Выполняем задачу --------------------------- #\n",
    "process(n = 3, r = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
